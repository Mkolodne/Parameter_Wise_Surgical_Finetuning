{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ae3a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a024f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "150e39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def read_data(path_dataset):\n",
    "    # train batch\n",
    "    train_batch = {}\n",
    "    for i in range(5):\n",
    "        filename = os.path.join(path_dataset, 'data_batch_{}'.format(i+1))\n",
    "        with open(filename, 'rb') as f:\n",
    "            try:\n",
    "                batch = pickle.load(f, encoding='bytes')\n",
    "            except TypeError:\n",
    "                batch = pickle.load(f) # for python 2\n",
    "            for key in batch.keys():\n",
    "                train_batch.setdefault(key, []).extend(batch[key])\n",
    "    train_batch = {k: np.stack(v, 0) for k, v in train_batch.items()} # stack into one batch\n",
    "\n",
    "    # test batch\n",
    "    filename = os.path.join(path_dataset, 'test_batch')\n",
    "    with open(filename, 'rb') as f:\n",
    "        try:\n",
    "            test_batch = pickle.load(f, encoding='bytes')\n",
    "        except TypeError:\n",
    "            test_batch = pickle.load(f)\n",
    "\n",
    "    # Reshape images: (n, 3072) -> (n, 32, 32, 3)\n",
    "    label_key = 'labels'.encode('utf-8')\n",
    "    train_images = np.transpose(\n",
    "        np.reshape(train_batch['data'.encode('utf-8')], [-1, 3, 32, 32]), [0,2,3,1])\n",
    "    train_labels = np.asarray(train_batch[label_key])\n",
    "    test_images = np.transpose(\n",
    "        np.reshape(test_batch['data'.encode('utf-8')], [-1, 3, 32, 32]), [0,2,3,1])\n",
    "    test_labels = np.asarray(test_batch[label_key])\n",
    "\n",
    "    # Pre-processing (normalize)\n",
    "    train_images = np.divide(train_images, 255, dtype=np.float32)\n",
    "    test_images = np.divide(test_images, 255, dtype=np.float32)\n",
    "    channel_mean = np.mean(train_images, axis=(0,1,2), dtype=np.float32, keepdims=True)\n",
    "    channel_std = np.std(train_images, axis=(0,1,2), dtype=np.float32, keepdims=True)\n",
    "    train_images = (train_images - channel_mean) / channel_std\n",
    "    test_images = (test_images - channel_mean) / channel_std\n",
    "\n",
    "    dataset = {\n",
    "        'train': {'input': train_images, 'label': train_labels},\n",
    "        'test': {'input': test_images, 'label': test_labels},\n",
    "    }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "327202bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Optional, depending on the model's expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "trainset = datasets.CIFAR10(root='/Users/matthewkolodner/Desktop/Stanford/CS330/Project/data/', train=True, download=False, transform=transform)\n",
    "testset = datasets.CIFAR10(root='/Users/matthewkolodner/Desktop/Stanford/CS330/Project/data/', train=False, download=False, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76770fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19f45395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                 datasource,\n",
    "                 arch,\n",
    "                 num_classes,\n",
    "                 target_sparsity,\n",
    "                 optimizer,\n",
    "                 lr_decay_type,\n",
    "                 lr,\n",
    "                 decay_boundaries,\n",
    "                 decay_values,\n",
    "                 initializer_w_bp,\n",
    "                 initializer_b_bp,\n",
    "                 initializer_w_ap,\n",
    "                 initializer_b_ap,\n",
    "                 **kwargs):\n",
    "        self.datasource = datasource\n",
    "        self.arch = arch\n",
    "        self.num_classes = num_classes\n",
    "        self.target_sparsity = target_sparsity\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_decay_type = lr_decay_type\n",
    "        self.lr = lr\n",
    "        self.decay_boundaries = decay_boundaries\n",
    "        self.decay_values = decay_values\n",
    "        self.initializer_w_bp = initializer_w_bp\n",
    "        self.initializer_b_bp = initializer_b_bp\n",
    "        self.initializer_w_ap = initializer_w_ap\n",
    "        self.initializer_b_ap = initializer_b_ap\n",
    "\n",
    "    def construct_model(self):\n",
    "        # Base-learner\n",
    "        self.net = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        weights = self.net.state_dict()\n",
    "        mask_init = mask_init = {k: var_no_train(tf.ones(weights[k].shape)) for k in prn_keys}\n",
    "        mask_init = {k: var_no_train(weights[k].shape) for k in weights}\n",
    "        mask_prev = {k: var_no_train(weights[k].shape) for k in weights}\n",
    "        \n",
    "        # Model\n",
    "\n",
    "        def get_sparse_mask():\n",
    "            w_mask = apply_mask(weights, mask_init)\n",
    "            logits = net.forward_pass(w_mask, self.inputs['input'],\n",
    "                self.is_train, trainable=False)\n",
    "            loss = tf.reduce_mean(compute_loss(self.inputs['label'], logits))\n",
    "            grads = tf.gradients(loss, [mask_init[k] for k in prn_keys])\n",
    "            gradients = dict(zip(prn_keys, grads))\n",
    "            cs = normalize_dict({k: tf.abs(v) for k, v in gradients.items()})\n",
    "            return create_sparse_mask(cs, self.target_sparsity)\n",
    "\n",
    "        mask = tf.cond(self.compress, lambda: get_sparse_mask(), lambda: mask_prev)\n",
    "        with tf.control_dependencies([tf.assign(mask_prev[k], v) for k,v in mask.items()]):\n",
    "            w_final = apply_mask(weights, mask)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net.forward_pass(w_final, self.inputs['input'], self.is_train)\n",
    "\n",
    "        # Loss\n",
    "        opt_loss = tf.reduce_mean(compute_loss(self.inputs['label'], logits))\n",
    "        reg = 0.00025 * tf.reduce_sum([tf.reduce_sum(tf.square(v)) for v in w_final.values()])\n",
    "        opt_loss = opt_loss + reg\n",
    "\n",
    "        # Optimization\n",
    "        optim, lr, global_step = prepare_optimization(opt_loss, self.optimizer, self.lr_decay_type,\n",
    "            self.lr, self.decay_boundaries, self.decay_values)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # TF version issue\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train_op = optim.minimize(opt_loss, global_step=global_step)\n",
    "\n",
    "        # Outputs\n",
    "        output_class = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        output_correct_prediction = tf.equal(self.inputs['label'], output_class)\n",
    "        output_accuracy_individual = tf.cast(output_correct_prediction, tf.float32)\n",
    "        output_accuracy = tf.reduce_mean(output_accuracy_individual)\n",
    "        self.outputs = {\n",
    "            'logits': logits,\n",
    "            'los': opt_loss,\n",
    "            'acc': output_accuracy,\n",
    "            'acc_individual': output_accuracy_individual,\n",
    "        }\n",
    "        self.sparsity = compute_sparsity(w_final, prn_keys)\n",
    "\n",
    "        # Summaries\n",
    "        tf.summary.scalar('loss', opt_loss)\n",
    "        tf.summary.scalar('accuracy', output_accuracy)\n",
    "        tf.summary.scalar('lr', lr)\n",
    "        self.summ_op = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "def compute_loss(labels, logits):\n",
    "    assert len(labels.shape)+1 == len(logits.shape)\n",
    "    num_classes = logits.shape.as_list()[-1]\n",
    "    labels = tf.one_hot(labels, num_classes, dtype=tf.float32)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "def get_optimizer(optimizer, lr):\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    elif optimizer == 'momentum':\n",
    "        optimizer = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return optimizer\n",
    "\n",
    "def prepare_optimization(loss, optimizer, lr_decay_type, learning_rate, boundaries, values):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    if lr_decay_type == 'constant':\n",
    "        learning_rate = tf.constant(learning_rate)\n",
    "    elif lr_decay_type == 'piecewise':\n",
    "        assert len(boundaries)+1 == len(values)\n",
    "        learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    optim = get_optimizer(optimizer, learning_rate)\n",
    "    return optim, learning_rate, global_step\n",
    "\n",
    "def vectorize_dict(x, sortkeys=None):\n",
    "    assert isinstance(x, dict)\n",
    "    if sortkeys is None:\n",
    "        sortkeys = x.keys()\n",
    "    def restore(v, x_shape, sortkeys):\n",
    "        # v splits for each key\n",
    "        split_sizes = []\n",
    "        for key in sortkeys:\n",
    "            split_sizes.append(functools.reduce(lambda x, y: x*y, x_shape[key]))\n",
    "        v_splits = tf.split(v, num_or_size_splits=split_sizes)\n",
    "        # x restore\n",
    "        x_restore = {}\n",
    "        for i, key in enumerate(sortkeys):\n",
    "            x_restore.update({key: tf.reshape(v_splits[i], x_shape[key])})\n",
    "        return x_restore\n",
    "    # vectorized dictionary\n",
    "    x_vec = tf.concat([tf.reshape(x[k], [-1]) for k in sortkeys], axis=0)\n",
    "    # restore function\n",
    "    x_shape = {k: x[k].shape.as_list() for k in sortkeys}\n",
    "    restore_fn = functools.partial(restore, x_shape=x_shape, sortkeys=sortkeys)\n",
    "    return x_vec, restore_fn\n",
    "\n",
    "def normalize_dict(x):\n",
    "    x_v, restore_fn = vectorize_dict(x)\n",
    "    x_v_norm = tf.divide(x_v, tf.reduce_sum(x_v))\n",
    "    x_norm = restore_fn(x_v_norm)\n",
    "    return x_norm\n",
    "\n",
    "def compute_sparsity(weights, target_keys):\n",
    "    assert isinstance(weights, dict)\n",
    "    w = {k: weights[k] for k in target_keys}\n",
    "    w_v, _ = vectorize_dict(w)\n",
    "    sparsity = tf.nn.zero_fraction(w_v)\n",
    "    return sparsity\n",
    "\n",
    "def create_sparse_mask(mask, target_sparsity):\n",
    "    def threshold_vec(vec, target_sparsity):\n",
    "        num_params = vec.shape.as_list()[0]\n",
    "        kappa = int(round(num_params * (1. - target_sparsity)))\n",
    "        topk, ind = tf.nn.top_k(vec, k=kappa, sorted=True)\n",
    "        mask_sparse_v = tf.sparse_to_dense(ind, tf.shape(vec),\n",
    "            tf.ones_like(ind, dtype=tf.float32), validate_indices=False)\n",
    "        return mask_sparse_v\n",
    "    if isinstance(mask, dict):\n",
    "        mask_v, restore_fn = vectorize_dict(mask)\n",
    "        mask_sparse_v = threshold_vec(mask_v, target_sparsity)\n",
    "        return restore_fn(mask_sparse_v)\n",
    "    else:\n",
    "        return threshold_vec(mask, target_sparsity)\n",
    "\n",
    "def apply_mask(weights, mask):\n",
    "    all_keys = weights.keys()\n",
    "    target_keys = mask.keys()\n",
    "    remain_keys = list(set(all_keys) - set(target_keys))\n",
    "    w_sparse = {k: mask[k] * weights[k] for k in target_keys}\n",
    "    w_sparse.update({k: weights[k] for k in remain_keys})\n",
    "    return w_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ece7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a34a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "target_sparsity = 0.95\n",
    "def var_no_train(shape):\n",
    "    return torch.ones(shape, dtype=torch.float32, requires_grad=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weights = model.state_dict()\n",
    "mask_init = {k: var_no_train(weights[k].shape) for k in weights}\n",
    "mask_prev = {k: var_no_train(weights[k].shape) for k in weights}\n",
    "w_mask = apply_mask(weights, mask_init)\n",
    "\n",
    "for inputs, labels in trainloader:\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    absolute_gradients = {name: param.grad.abs() for name, param in model.named_parameters()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "711a0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore(v, x_shape, sortkeys):\n",
    "    split_sizes = []\n",
    "    for key in sortkeys:\n",
    "        split_sizes.append(functools.reduce(lambda x, y: x*y, x_shape[key]))\n",
    "    v_splits = torch.split(v, split_size_or_sections=split_sizes)\n",
    "    x_restore = {}\n",
    "    for i, key in enumerate(sortkeys):\n",
    "        x_restore.update({key: torch.reshape(v_splits[i], x_shape[key])})\n",
    "    return x_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9dba1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_v = torch.cat([absolute_gradients[k].view(-1) for k in absolute_gradients], dim=0)\n",
    "x_shape = {k: absolute_gradients[k].shape for k in absolute_gradients}\n",
    "restore_fn = functools.partial(restore, x_shape=x_shape, sortkeys=absolute_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "27aed2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = mask_v.shape[0]\n",
    "kappa = int(round(num_params * (1. - target_sparsity)))\n",
    "topk, ind = torch.topk(mask_v, k=kappa, largest=True, sorted=True)\n",
    "mask_sparse_v = torch.zeros_like(mask_v, dtype=torch.float32)\n",
    "mask_sparse_v = mask_sparse_v.scatter_(0, ind, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c09db392",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_final = restore_fn(mask_sparse_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965cdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6d1ccf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # or any other optimizer\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8e6fc269",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-b474ff39404d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param, mask in zip(model.parameters(), mask_final):\n",
    "                param.grad *= mask\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7cc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
